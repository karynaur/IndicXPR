{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading words mapping from ./data/train/train-en-fr-32-phrase.txt\n",
      "Loading words mapping from ./data/dev/dev-en-fr-32-phrase.txt\n",
      "Loading words mapping from ./data/test/test-en-fr-32-phrase.txt\n",
      "Loading tsv from ./data/sentences/en-fr-phrase-sentences.32.tsv ...\n",
      "Loading tsv from ./data/sentences/fr-phrase-sentences.32.tsv ...\n",
      "[!] collect 1315 samples\n",
      "没找到共0\n",
      "[!] collect 438 samples\n",
      "没找到共0\n",
      "[!] collect 438 samples\n",
      "没找到共0\n",
      "/home/karynaur/.local/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/karynaur/.local/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "cpu\n",
      "Namespace(T_para=0.06, adam_epsilon=1e-08, adapt_to_dataset=0, all_sentence_num=32, cut_type='eos-eos', data_dir='./data/', dev_all_sentence_num=32, dev_only_q_encoder=1, dev_phrase_path='./data/dev/dev-en-fr-32-phrase.txt', dev_sample_num=32, distributed=False, do_eval=True, do_lower_case=False, do_predict=False, do_train=False, eval_batch_size=32, eval_on='dev', fp16=False, fp16_opt_level='O1', gpu_id='0', gradient_accumulation_steps=4, is_type=-1, layer_id=12, learning_rate=2e-05, lg='fr', local_rank=1, loss_scale=0, max_grad_norm=1.0, max_seq_length=512, max_train_steps=None, model_name_or_path='xlm-roberta-base', momentum=0.999, no_cuda=False, num_train_epochs=18, num_warmup_steps=0, output_dir='./output/', output_log_dir='result', output_loss_dir='./result/4-fr-32-true-0-0.06-42-18-0.999-0-dev_qq-layer_12', output_model_path='./result/4-fr-32-true-0-0.06-42-18-0.999-0-dev_qq-layer_12/best.pt', pad_to_max_length=False, queue_length=0, seed=42, sentence_max_len=80, simclr=0, sn='32', src_context_path='./data/sentences/en-fr-phrase-sentences.32.tsv', test_on_dev=0, test_phrase_path='./data/test/test-en-fr-32-phrase.txt', train_batch_size=16, train_phrase_path='./data/train/train-en-fr-32-phrase.txt', train_sample_num=4, trg_context_path='./data/sentences/fr-phrase-sentences.32.tsv', unsupervised=0, warmup_proportion=0.1, weight_decay=0.0001, wo_span_eos='true', wolinear=0, z='./result/4fr-32/best.pt')\n",
      "MoCo(\n",
      "  (encoder_q): BackBone_Model(\n",
      "    (model): XLMRobertaModel(\n",
      "      (embeddings): XLMRobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): XLMRobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): XLMRobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (linear_head): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (criterion): CrossEntropyLoss()\n",
      "  )\n",
      "  (encoder_k): BackBone_Model(\n",
      "    (model): XLMRobertaModel(\n",
      "      (embeddings): XLMRobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): XLMRobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): XLMRobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (linear_head): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (criterion): CrossEntropyLoss()\n",
      "  )\n",
      ")\n",
      "epoch: 0\n",
      "/home/karynaur/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|                                                    | 0/82 [00:00<?, ?it/s]/home/karynaur/.local/lib/python3.8/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/home/karynaur/.local/lib/python3.8/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "!python3 trainMoCo.py \\\n",
    "    --lg 'fr' \\\n",
    "    --sn '32' \\\n",
    "    --simclr 0 \\\n",
    "    --queue_length 0 \\\n",
    "    --T_para 0.06 \\\n",
    "    --seed 42 \\\n",
    "    --output_log_dir 'result' \\\n",
    "    --dev_only_q_encoder 1 \\\n",
    "    --local_rank 1 \\\n",
    "    --distributed False \\\n",
    "    --train_batch_size 16  \\\n",
    "    --eval_batch_size 32 \\\n",
    "    --num_train_epochs 18 \\\n",
    "    --do_eval \\\n",
    "    --distributed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
